{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, so I don't have feelings or emotions like humans do, but thank you for asking! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "messages = [{ \"content\": \"There's a llama in my garden ðŸ˜± What should I do?\",\"role\": \"user\"}]\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/llama3.2\",\n",
    "    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of the latest research trends, several key areas have garnered significant attention in the field of artificial intelligence:\n",
      "\n",
      "1. **Generative AI and Large Language Models (LLMs):** The development and application of generative AI, particularly LLMs like OpenAI's GPT series and Google's BERT, remain at the forefront. These models are being refined for better accuracy, efficiency, and cost-effectiveness. Research also focuses on improving their adaptability to specific tasks via fine-tuning and instruction-following.\n",
      "\n",
      "2. **Multimodal AI:** There is an increasing interest in models that can process and integrate multiple types of data (text, images, audio, etc.). Projects like OpenAIâ€™s DALL-E, Google's DeepMind developments, and others are leading advancements in this area, aiming for models with more human-like understanding and capabilities.\n",
      "\n",
      "3. **Ethical AI and Bias Mitigation:** Researchers are actively working on addressing issues of bias, fairness, and privacy. There is a push for developing frameworks and tools for evaluating and mitigating biases in AI systems, as well as ensuring ethical use and deployment.\n",
      "\n",
      "4. **AI in Healthcare:** AI applications in healthcare are witnessing rapid growth, from drug discovery and personalized medicine to advanced diagnostic tools. Research emphasizes leveraging AI to improve healthcare outcomes while addressing challenges like data privacy and regulatory compliance.\n",
      "\n",
      "5. **Reinforcement Learning (RL):** RL continues to be a hot topic, especially in areas requiring decision-making under uncertainty. Advancements aim at improving sample efficiency, stability, and scaling of RL systems to more complex and realistic scenarios.\n",
      "\n",
      "6. **AI for Scientific Discovery:** AI's role in accelerating scientific research is expanding, with models being employed to solve complex problems in physics, chemistry, and climate science. This includes efforts to improve data processing from experiments and simulations.\n",
      "\n",
      "7. **AI and Edge Computing:** There's a growing focus on deploying AI models on edge devices, which requires innovations in making AI models smaller, more efficient, and capable of operating with limited computational resources.\n",
      "\n",
      "8. **Neurosymbolic AI:** Combining neural networks with symbolic reasoning to create hybrid models that leverage the strengths of both approaches is an emerging trend. This aims to enhance the explainability and robustness of AI systems.\n",
      "\n",
      "9. **AI for Sustainability:** AI research is increasingly directed towards sustainability, including optimizing energy consumption in data centers, developing models to predict environmental changes, and aiding in the management of renewable resources.\n",
      "\n",
      "10. **Human-AI Collaboration:** Enhancing interfaces and interaction paradigms that allow for more natural collaboration between humans and AI systems is an important area, improving usability and effectiveness in diverse applications.\n",
      "\n",
      "These trends reflect a broad and accelerating effort to enhance AI's capabilities while addressing critical ethical, technical, and societal challenges.\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "\n",
    "response = completion(\n",
    "    model=\"gpt-4o\",  # Using Anthropic's Claude model\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the latest AI research trends.\"}]\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.embedding(model='ollama/nomic-embed-text', api_base='http://localhost:11434', input=['good morning from litellm'], stream=False)\u001b[0m\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {'stream': False}\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:113 - `logging_obj` not found - unable to track `llm_api_duration_ms\n",
      "\u001b[92m22:26:59 - LiteLLM:INFO\u001b[0m: utils.py:1120 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:563 - completion_response _select_model_name_for_cost_calc: ollama/nomic-embed-text\n",
      "\u001b[92m22:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:4246 - checking potential_model_names in litellm.model_cost: {'split_model': 'nomic-embed-text', 'combined_model_name': 'ollama/nomic-embed-text', 'stripped_model_name': 'nomic-embed-text', 'combined_stripped_model_name': 'ollama/nomic-embed-text', 'custom_llm_provider': 'ollama'}\n",
      "\u001b[92m22:27:01 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:330 - Returned custom cost for model=ollama/nomic-embed-text - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m22:27:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:848 - response_cost: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingResponse(model='ollama/nomic-embed-text', data=[{'object': 'embedding', 'index': 0, 'embedding': [-0.050163407, -0.046852, -0.16544843, -0.039870854, 0.007908391, 0.012927727, -0.041993868, -0.0111064175, -0.020217868, -0.04302419, 0.043910135, 0.04686127, 0.083592236, 0.019195084, -0.002451764, -0.014500182, 0.0599147, 0.011103147, -0.023140008, -0.029087206, -0.00678627, -0.008281777, -0.025452485, -0.08328225, 0.06259651, -0.018840114, 0.03897058, 0.05518675, -0.011492649, -0.010283953, 0.085131176, -0.05599041, -0.042505562, -0.064653814, -0.026677448, 0.0044814316, 0.022364289, -0.034344673, -0.0063825985, 0.04245895, 0.09007466, -0.008859602, 0.022926277, 0.015453081, 0.06972764, 0.00881921, -0.03676003, 0.024527892, 0.022729527, -0.06947722, 0.0060004424, -0.056228876, -0.005134052, -0.018209727, 0.019688245, 0.04084222, 0.06676475, -0.012657645, 0.031752717, 0.024298152, 0.11120974, -0.0017264348, -0.021968516, 0.05338678, -0.012431894, -0.007820984, 0.023772318, 0.032274738, -0.017721552, -0.011325372, 0.038861807, -0.009039621, -0.032281943, -0.02292099, -0.025401665, 0.024404787, 0.011690793, -0.024995092, 0.004700589, -0.013653726, 0.0009027317, -0.006429068, 0.006672773, -0.015106075, 0.014955389, -0.0001000346, 0.0029303178, -0.00243775, -0.033527546, 0.0041584997, 0.09848711, 0.018542673, 0.043417815, -0.011493018, -0.090908885, 0.019246357, 0.0017007225, 0.042445336, -0.0255309, -0.06515729, 0.06098293, -0.039680164, -0.029303003, 0.04977345, 0.08670505, 0.0266008, -0.02708393, -0.06755607, -0.028949069, -0.017834844, -0.029574653, -0.0076727043, -0.024678335, -0.100963086, 0.054808494, -0.052870993, 0.07290677, -0.059060313, 0.06680429, 0.014173063, -0.013956926, -0.051088337, 0.061504044, -0.041420586, 0.0648494, 0.014706886, -0.025551166, -0.04276324, -0.020970605, -0.02388734, 0.043469436, -0.032359473, -0.034263, -0.008352863, 0.008905091, 0.058394026, 0.026507834, -0.021820473, 0.08059039, 0.0021553633, 0.025722483, 0.010418591, -0.004315654, -0.008774514, 0.013977771, 0.04267131, 0.018069344, -0.044500273, -0.00022403745, 0.028005835, -0.042519346, -0.019906247, 0.05635722, 0.04035138, 0.028544577, -0.048804656, -0.03240081, 0.001990223, 0.022789426, -0.020599568, 0.021791462, -0.008884594, -0.018858619, -0.020288272, -0.027883036, -0.053399287, 0.004408954, 0.061403476, 0.01634094, 0.027214129, -0.037199486, 0.030028686, -0.017242182, -0.054892395, 0.026571812, -0.009706103, -0.030912912, -0.032628283, 0.008321378, -0.024909325, 0.010093884, -0.030516552, -0.005795248, 0.02826615, -0.0046818973, -0.06286864, 0.022191362, -0.032491826, -0.06277174, -0.03829208, 0.017929506, 0.018348299, -0.041896746, -0.039641447, -0.031281605, -0.022970837, 0.027322337, 0.02595945, 0.06616942, -0.026655799, -0.0028446233, -0.017140783, -0.023980219, 0.012965141, -0.042444352, 0.08057487, -0.04481069, 0.05870685, 0.020712785, 0.01687214, 0.03256994, -0.02588626, -0.014992133, -0.025165463, -0.0031561935, 0.009045662, 0.0014103572, 0.026493335, -0.0017592928, 0.020797752, 0.0012603126, 0.044304468, -0.003865435, -0.03718361, 0.05256157, -0.04553, -0.021353666, -0.021997536, -0.05551136, 0.03845297, -0.033772368, -0.02805523, 0.038821857, 0.019275794, 0.053456083, 0.024526332, -0.0035601936, 0.045781437, 0.010169247, -0.040581405, -0.01312126, 0.03801166, -0.0012625755, -0.023550855, -0.06796854, 0.037891887, 0.016805407, 0.00022614877, -0.027328463, 0.037259065, 0.009970194, -0.010195449, 0.015759451, -0.0026720427, 0.007130219, -0.07599845, -0.03401315, 0.034864414, -0.0104342485, -0.010525748, -0.00082629494, -0.07120632, -0.0012411509, -0.060965847, 0.019317986, 0.049596917, -0.0028783, 0.039946306, 0.007215509, -0.064515546, 0.025823655, 0.013171126, 0.064893976, 0.089830354, 0.002974962, -0.052242573, -0.08230123, 0.026694056, -0.06271084, -0.005977285, 0.018685704, -0.05248915, -0.023212716, -0.015905429, -0.0076933797, 0.03394845, 0.023893218, 0.014136038, 0.010771712, -0.032974433, 0.032995056, 0.037971113, 0.017204823, 0.0192663, -0.025333676, -0.03682445, 0.039651383, 0.019096458, 0.0013440384, -0.05430392, 0.02337862, 0.009406591, 0.0074379356, 0.014344769, -0.027023362, 0.0442843, -0.029051267, 0.017573291, 0.048015837, 0.014201565, -0.016129842, -0.034648504, -0.02725282, -0.021764632, -0.032436583, 0.047556955, -0.01276675, 0.003400939, 0.04141438, 0.014169805, -0.009187415, -0.031595543, -0.03584521, -0.058676455, -0.013336052, -0.003701371, 0.027257413, 0.06850856, 0.043767244, -0.013237477, -0.0009374707, 0.03813669, -0.010631557, -0.0023680066, -0.00123682, 0.021222744, -0.023988863, -0.025217984, -0.016512094, 0.018225584, -0.016256709, -0.007141655, 0.006284276, 0.0015871128, -0.038649112, -0.0076989187, -0.051049482, -0.03236507, -0.0053832983, 0.0010619782, -0.018558301, 0.014688436, -0.034475327, 0.01795362, -0.007651877, 0.016059313, 0.007768748, 0.057480946, 0.10243124, -0.011754359, -0.0016244885, 0.010487141, -0.00024783963, -0.049418345, -0.0056473673, 0.02854681, 0.03544619, 0.011264295, 0.010233605, 0.037032224, -0.0058298497, -0.008175739, -0.017140068, 0.037914798, 0.06861669, 0.039683495, -0.06706114, -0.017616084, -0.058311578, 0.005415298, -0.00686146, -0.02662744, 0.02592326, -0.021943012, 0.007135375, 0.057304565, 0.05144034, 0.025223283, -0.046323393, 0.026305571, 0.025975619, -0.054448385, -0.03553077, 0.054926217, 0.028943952, 0.008795628, 0.09024716, 0.006354656, -0.009412502, 0.030396726, 0.017199721, -0.021041902, 0.025994433, -0.034863345, -0.037752, -0.002652166, -0.034303695, -0.0012191578, 0.030904405, -0.046614576, 0.023461895, 0.032753386, 0.009930901, -0.035975914, -0.020165691, 0.02003186, 0.0064080404, -0.025444329, 0.0070552584, 0.05833631, 0.03803375, 0.067486316, -0.016586684, -0.012391784, 0.03317893, 0.03655971, 0.085403636, -0.037375778, -0.038099326, -0.08297617, 0.026728472, 0.026354795, 0.009414456, 0.03828219, 0.002316473, -0.039940216, -0.013785425, 0.05063967, 0.012058878, 0.008096718, 0.06020326, -0.0735662, -0.023541657, 0.012591141, 0.02154302, 0.097554885, 0.07585423, -0.0072597894, -0.049480382, 0.014464517, 0.0019877732, 0.019676128, 0.024569988, -0.0089577325, 0.072180055, -0.0010649994, 0.05950652, 0.04519138, 0.036678694, 0.041372035, 0.054756816, -0.0018643517, -0.035428718, -0.01629027, 0.03873964, -0.024026202, -0.020500442, 0.019311527, 0.010512366, 0.0031164824, -0.027762236, 0.020569954, 0.018914575, -0.060936905, -0.06469399, 0.031502128, -0.04579152, -0.010860819, -0.020025155, 0.03505287, 0.020112094, -0.02006828, -0.04291693, -0.07618456, 0.02366469, 0.080503054, 0.039472174, 0.013371094, -0.015477155, -0.04912013, 0.02170368, 0.02924463, 0.04089129, -0.022865849, 0.0088535845, 0.010390054, -0.045351617, 0.048608303, 0.0064212456, 0.0008920997, 0.011909009, 0.036230642, -0.0048062094, -0.0098929955, 0.004795307, 0.0027327386, -0.04151628, -0.007942681, -0.011606271, -0.03393815, -0.021474225, 0.005848498, -0.006633282, 0.045753926, 0.035226252, -0.08183663, 0.027193503, 0.025601259, -0.0017963717, 0.055276953, -0.029132843, -0.038197625, 0.011528074, -0.011639288, -0.0090765385, 0.035535473, -0.026348613, -0.012757122, -0.02819077, -0.042373367, 0.0137713915, 0.014928731, 0.008122588, -0.04931877, 0.04143734, -0.03590628, 0.016808143, 0.039160613, 0.008482014, 0.038117245, -0.024942538, -0.035978608, -0.012022946, -0.013479965, 0.032916795, -0.0052680993, -0.07346257, -0.011607113, 0.014075762, -0.03126792, 0.049585804, -0.086278774, 0.033763442, -0.0028926828, -0.0023840426, -0.060500603, -0.068041205, -0.023367768, -0.03432631, 0.035252232, 0.062256843, -0.029034195, 0.049081042, 0.01371882, 0.040383913, -0.027685372, -0.020079285, -0.02752489, 0.020693434, -0.018389296, 0.009821049, -0.061316945, -0.04249656, 0.020547926, -0.0015186628, -0.026117215, 0.010026395, -0.009666089, -0.053513866, -0.029645238, -0.01883748, -0.021121552, 0.048790652, 0.00036591056, -0.029760111, -0.009846256, -0.028141996, -0.011788957, 0.054769427, -0.0400718, 0.02715731, 0.010058557, -0.028647648, 0.006684757, -0.0104578715, 0.016966438, -0.037575603, -0.048142806, -0.01202321, -0.022273835, -0.030713525, -0.006637942, 0.0497689, -0.012652706, -0.008017411, 0.038664415, -0.0043976973, 0.019220963, 0.031716168, 0.00011843365, 0.034497343, 0.024201915, -0.046070665, 0.025373613, 0.021580726, 0.022264486, 0.01310775, -0.0056134663, -0.021088634, -0.057035435, -0.001689345, -0.041889567, 0.0122941565, 0.019989107, -0.03127354, 0.035724577, -0.03555764, -0.03331303, -0.05890741, 0.08572067, 0.0081491815, -0.048500452, -0.038399074, -0.05515978, -0.010102573, 0.016984738, 0.03498192, -0.025942989, -0.009062168, 0.037183188, 0.04774058, -0.01968601, 0.0156253, 0.026810383, 0.0029010912, -0.0005484181, 0.019647354, 0.013230331, 0.074872784, 0.027352946, 0.06593859, 0.02410929, 0.047052372, 0.016953226, -0.0054063783, -0.024233589, 0.018922187, -0.06209186, -0.013661614, -0.065631874, -0.011589997, -0.046713773, -0.04789482, -0.03882442, 0.011514599, 0.005330155, -0.057777505, -0.0058723674, -0.04104465, -0.029422725, 0.004634153, 0.0058017545, 0.020604113, -0.0016088029, -0.049243476, 0.054941002, 0.014185614, 0.009103598, 0.017655615, -0.0070166527, -0.026765028, 0.004421695, -0.054456078, 0.023993112, 0.029115133, 0.010603964, 0.056640808, 0.0060334015, 0.0015572474, -0.040523756, 0.016250018, -0.020800233, -0.037004907, -0.014371796, -0.012372516, 0.020149557, 0.017145121, 0.00086789107, 0.0123713175, 0.0648757, -0.044274535, 0.0073125977, 0.011657576, 0.023156788, -0.028916603, 0.01835942, 0.01577388, 0.012829781, 0.016580509, -0.014672598, -0.084320135, -0.0059201308, -0.020903736, 0.039065864, 0.07668403, 0.008193445, 0.024943678, 0.032367013, 0.017784858, 0.07726476, 0.069300406, -0.030514395, -0.041023143, 0.0076343818, 0.019046754, 0.019282283, 0.012828578, -0.043324538, 0.0048153247, -0.045973033, 0.040914904, -0.052281994, -0.011694009, 0.071756035, -0.0019105463, 0.025658987, -0.035610236, 0.011087235, -0.01782423, -0.0017804096, -0.05804933, -0.0451199, -0.0126021765, 0.0139680095, -0.0068127243, -0.013203836, 0.012354032, 0.00776627, -0.017389352, -0.020415204, 0.00015998623, 0.016356334, -0.022552392, -0.0026367796, 0.011111369, 0.029782297, 0.032449573, -0.012886407, 0.04785289, -0.03328552, 0.028846936, -0.056408275, 0.06895269, 0.029208403, -0.048421856, -0.004641799, -0.07983869, 0.018467499]}], object='list', usage=Usage(completion_tokens=6, prompt_tokens=6, total_tokens=6, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm import embedding\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "litellm._turn_on_debug()\n",
    "\n",
    "response = embedding(\n",
    "    model='ollama/nomic-embed-text',\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    input=[\"good morning from litellm\"],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.completion(model='ollama/llama3.2', messages=[{'content': 'Write some python code to print the contents of a file.', 'role': 'user'}], stream=False)\u001b[0m\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: logging_callback_manager.py:189 - Custom logger of type MyCustomHandler, key: MyCustomHandler-message_logging=True already exists in [<__main__.MyCustomHandler object at 0x000002E09B05BB50>], not adding again..\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: logging_callback_manager.py:189 - Custom logger of type MyCustomHandler, key: MyCustomHandler-message_logging=True already exists in [<__main__.MyCustomHandler object at 0x000002E09B05BB50>], not adding again..\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: logging_callback_manager.py:189 - Custom logger of type MyCustomHandler, key: MyCustomHandler-message_logging=True already exists in [<__main__.MyCustomHandler object at 0x000002E09B05BB50>], not adding again..\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: logging_callback_manager.py:189 - Custom logger of type MyCustomHandler, key: MyCustomHandler-message_logging=True already exists in [<__main__.MyCustomHandler object at 0x000002E09B05BB50>], not adding again..\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Initialized litellm callbacks, Async Success Callbacks: [<__main__.MyCustomHandler object at 0x000002E09B05BB50>]\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: transformation.py:124 - Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m23:18:32 - LiteLLM:INFO\u001b[0m: utils.py:2959 - \n",
      "LiteLLM completion() model= llama3.2; provider = ollama\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:2962 - \n",
      "LiteLLM: Params passed to completion() {'model': 'llama3.2', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Write some python code to print the contents of a file.', 'role': 'user'}]}\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:2965 - \n",
      "LiteLLM: Non-Default params passed to completion() {'stream': False}\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Final returned optional params: {'stream': False}\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {'stream': False}\n",
      "\u001b[92m23:18:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:636 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llama3.2', 'prompt': '### User:\\nWrite some python code to print the contents of a file.\\n\\n', 'options': {}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-API Call\n",
      "Pre-API Call\n",
      "Pre-API Call\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m23:18:39 - LiteLLM:INFO\u001b[0m: utils.py:1120 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m23:18:39 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:563 - completion_response _select_model_name_for_cost_calc: ollama/llama3.2\n",
      "\u001b[92m23:18:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4246 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama3.2', 'combined_model_name': 'ollama/llama3.2', 'stripped_model_name': 'llama3.2', 'combined_stripped_model_name': 'ollama/llama3.2', 'custom_llm_provider': 'ollama'}\n",
      "\u001b[92m23:18:41 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:330 - Returned custom cost for model=ollama/llama3.2 - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m23:18:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:848 - response_cost: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Printing File Contents in Python\n",
      "\n",
      "You can use the following Python function to read and print the contents of a file:\n",
      "\n",
      "```python\n",
      "def print_file_contents(file_path):\n",
      "    \"\"\"\n",
      "    Reads and prints the contents of a specified file.\n",
      "    \n",
      "    Args:\n",
      "        file_path (str): The path to the file you want to read.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If the specified file does not exist.\n",
      "    \"\"\"\n",
      "\n",
      "    try:\n",
      "        # Open the file in read mode\n",
      "        with open(file_path, 'r') as file:\n",
      "            # Read the contents of the file\n",
      "            file_contents = file.read()\n",
      "            \n",
      "            # Print the contents of the file\n",
      "            print(file_contents)\n",
      "        \n",
      "    except FileNotFoundError:\n",
      "        print(f\"The file {file_path} does not exist.\")\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "\n",
      "# Example usage:\n",
      "print_file_contents('path_to_your_file.txt')\n",
      "```\n",
      "\n",
      "Make sure to replace `'path_to_your_file.txt'` with the actual path to your desired file.\n",
      "\n",
      "### Alternative Using `with open()` and `read()`\n",
      "\n",
      "Alternatively, you can use a `try-except` block without the `with open()` statement. Here's how:\n",
      "\n",
      "```python\n",
      "def print_file_contents(file_path):\n",
      "    try:\n",
      "        # Open the file in read mode\n",
      "        file = open(file_path, 'r')\n",
      "        \n",
      "        # Read the contents of the file\n",
      "        file_contents = file.read()\n",
      "        \n",
      "        # Print the contents of the file\n",
      "        print(file_contents)\n",
      "    \n",
      "    except FileNotFoundError as e:\n",
      "        print(f\"The file {file_path} does not exist.\")\n",
      "    \n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "```\n",
      "\n",
      "However, using `with open()` is generally recommended because it automatically closes the file when you're done with it, even if an exception occurs.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm.integrations.custom_logger import CustomLogger\n",
    "from litellm import completion\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                    filemode='w',\n",
    "                    filename='litellm.log')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Custom Logger \n",
    "class MyCustomHandler(CustomLogger):\n",
    "    def log_pre_api_call(self, model, messages, kwargs): \n",
    "        logger.info(f\"Pre-API Call\")\n",
    "    def log_post_api_call(self, kwargs, response_obj, start_time, end_time): \n",
    "        logger.info(f\"Post-API Call\")\n",
    "    def log_success_event(self, kwargs, response_obj, start_time, end_time): \n",
    "        logger.info(f\"On Success\")\n",
    "    def log_failure_event(self, kwargs, response_obj, start_time, end_time): \n",
    "        logger.info(f\"On Failure\")\n",
    "\n",
    "# initiallize the handeler\n",
    "customHandler = MyCustomHandler()\n",
    "# pass the handler to the callback\n",
    "litellm.callbacks = [customHandler]\n",
    "\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/llama3.2\",\n",
    "    messages=[{ \"content\": \"Write some python code to print the contents of a file.\",\"role\": \"user\"}],\n",
    "    stream=False\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.completion(model='huggingface/Qwen/Qwen2.5-VL-7B-Instruct', messages=[{'content': 'Hello, how are you?', 'role': 'user'}], stream=False)\u001b[0m\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: transformation.py:124 - Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m14:54:22 - LiteLLM:INFO\u001b[0m: utils.py:2959 - \n",
      "LiteLLM completion() model= Qwen/Qwen2.5-VL-7B-Instruct; provider = huggingface\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:2962 - \n",
      "LiteLLM: Params passed to completion() {'model': 'Qwen/Qwen2.5-VL-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Hello, how are you?', 'role': 'user'}]}\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:2965 - \n",
      "LiteLLM: Non-Default params passed to completion() {'stream': False}\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Final returned optional params: {'stream': False}\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {'stream': False}\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:636 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-VL-7B-Instruct \\\n",
      "-H 'content-type: *****' -H 'Authorization: *****' \\\n",
      "-d '{'inputs': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\n', 'parameters': {'stream': False, 'details': True, 'return_full_text': False}, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:63 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=Qwen/Qwen2.5-VL-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m14:54:22 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2231 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m14:54:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1843 - Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceUnavailableError",
     "evalue": "litellm.ServiceUnavailableError: HuggingfaceException - {\"error\":\"Model Qwen/Qwen2.5-VL-7B-Instruct is currently loading\",\"estimated_time\":663.3733520507812}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\huggingface\\chat\\handler.py:239\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, model_response, print_verbose, timeout, encoding, api_key, logging_obj, optional_params, litellm_params, custom_prompt_dict, acompletion, logger_fn, client, headers)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m### SYNC COMPLETION\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompletion_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hf_chat_config\u001b[38;5;241m.\u001b[39mtransform_response(\n\u001b[0;32m    246\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    247\u001b[0m         raw_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         litellm_params\u001b[38;5;241m=\u001b[39mlitellm_params,\n\u001b[0;32m    257\u001b[0m     )\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:557\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:538\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    537\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m--> 538\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Server error '503 Service Unavailable' for url 'https://api-inference.huggingface.co/models/Qwen/Qwen2.5-VL-7B-Instruct'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mHuggingfaceError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\main.py:2116\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2115\u001b[0m custom_prompt_dict \u001b[38;5;241m=\u001b[39m custom_prompt_dict \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcustom_prompt_dict\n\u001b[1;32m-> 2116\u001b[0m model_response \u001b[38;5;241m=\u001b[39m \u001b[43mhuggingface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhuggingface_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optional_params\n\u001b[0;32m   2136\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m optional_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2138\u001b[0m ):\n\u001b[0;32m   2139\u001b[0m     \u001b[38;5;66;03m# don't try to access stream object,\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\huggingface\\chat\\handler.py:259\u001b[0m, in \u001b[0;36mHuggingface.completion\u001b[1;34m(self, model, messages, api_base, model_response, print_verbose, timeout, encoding, api_key, logging_obj, optional_params, litellm_params, custom_prompt_dict, acompletion, logger_fn, client, headers)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HuggingfaceError(\n\u001b[0;32m    260\u001b[0m         status_code\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[0;32m    261\u001b[0m         message\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m    262\u001b[0m         headers\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HuggingfaceError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mHuggingfaceError\u001b[0m: {\"error\":\"Model Qwen/Qwen2.5-VL-7B-Instruct is currently loading\",\"estimated_time\":663.3733520507812}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a llama in my garden ðŸ˜± What should I do?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# e.g. Call 'https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO' from Serverless Inference API\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1187\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1189\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1069\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\main.py:3085\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   3082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3084\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:1496\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[0;32m   1495\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailableError(\n\u001b[0;32m   1497\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingfaceException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_exception\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1498\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1499\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1500\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1501\u001b[0m     )\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mServiceUnavailableError\u001b[0m: litellm.ServiceUnavailableError: HuggingfaceException - {\"error\":\"Model Qwen/Qwen2.5-VL-7B-Instruct is currently loading\",\"estimated_time\":663.3733520507812}"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "\n",
    "litellm._turn_on_debug()\n",
    "MODEL_NAME = \"huggingface/Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "\n",
    "messages = [{ \"content\": \"There's a llama in my garden ðŸ˜± What should I do?\",\"role\": \"user\"}]\n",
    "response = completion(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
