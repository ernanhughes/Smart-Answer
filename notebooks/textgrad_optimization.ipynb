{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrad as tg\n",
    "tg.set_backward_engine(tg.get_engine(\"gpt-4o\"))\n",
    "\n",
    "initial_solution = \"\"\"To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
    "x = (-b ± √(b^2 - 4ac)) / 2a\n",
    "a = 3, b = -7, c = 2\n",
    "x = (7 ± √((-7)^2 - 4 * 3(2))) / 6\n",
    "x = (7 ± √(7^3) / 6\n",
    "The solutions are:\n",
    "x1 = (7 + √73)\n",
    "x2 = (7 - √73)\"\"\"\n",
    "\n",
    "# Define the variable to optimize, let requires_grad=True to enable gradient computation\n",
    "solution = tg.Variable(initial_solution,\n",
    "                       requires_grad=True,\n",
    "                       role_description=\"solution to the math question\")\n",
    "\n",
    "# Define the loss function, via a system prompt to an LLM\n",
    "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math question. Do not attempt to solve it yourself, do not give a solution, only identify errors. Be super concise.\"\"\",\n",
    "                                 requires_grad=False,\n",
    "                                 role_description=\"system prompt\")\n",
    "\n",
    "loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "\n",
    "# Define the optimizer, let the optimizer know which variables to optimize\n",
    "optimizer = tg.TGD(parameters=[solution])\n",
    "\n",
    "loss = loss_fn(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textgrad as tg\n",
    "from textgrad import get_engine, set_backward_engine\n",
    "\n",
    "MODEL_NAME = \"ollama/Qwen2.5\"\n",
    "\n",
    "engine = get_engine(f\"experimental:{MODEL_NAME}\", cache=False)\n",
    "# this also works with\n",
    "set_backward_engine(f\"experimental:{MODEL_NAME}\", cache=False, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.completion(model='ollama/llava', messages=[{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}])\u001b[0m\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: transformation.py:124 - Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m18:12:07 - LiteLLM:INFO\u001b[0m: utils.py:2959 - \n",
      "LiteLLM completion() model= llava; provider = ollama\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:2962 - \n",
      "LiteLLM: Params passed to completion() {'model': 'llava', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}]}\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:2965 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Final returned optional params: {}\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:636 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llava', 'prompt': \"you are an assistanthello, what's 3+4\", 'options': {}, 'stream': False, 'images': []}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m18:12:09 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:63 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llava\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m18:12:09 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2231 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:12:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1843 - Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.completion(model='ollama/llava', messages=[{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}])\u001b[0m\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: transformation.py:124 - Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m18:12:10 - LiteLLM:INFO\u001b[0m: utils.py:2959 - \n",
      "LiteLLM completion() model= llava; provider = ollama\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:2962 - \n",
      "LiteLLM: Params passed to completion() {'model': 'llava', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}]}\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:2965 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Final returned optional params: {}\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:636 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llava', 'prompt': \"you are an assistanthello, what's 3+4\", 'options': {}, 'stream': False, 'images': []}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:63 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llava\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2231 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:12:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1843 - Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \u001b[92mlitellm.completion(model='ollama/llava', messages=[{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}])\u001b[0m\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - \n",
      "\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: transformation.py:124 - Translating developer role to system role for non-OpenAI providers.\n",
      "\u001b[92m18:12:11 - LiteLLM:INFO\u001b[0m: utils.py:2959 - \n",
      "LiteLLM completion() model= llava; provider = ollama\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:2962 - \n",
      "LiteLLM: Params passed to completion() {'model': 'llava', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'ollama', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'you are an assistant'}, {'role': 'user', 'content': \"hello, what's 3+4\"}]}\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:2965 - \n",
      "LiteLLM: Non-Default params passed to completion() {}\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: utils.py:301 - Final returned optional params: {}\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:379 - self.optional_params: {}\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:636 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "http://localhost:11434/api/generate \\\n",
      "-d '{'model': 'llava', 'prompt': \"you are an assistanthello, what's 3+4\", 'options': {}, 'stream': False, 'images': []}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:63 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llava\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2231 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m18:12:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1843 - Logging Details LiteLLM-Failure Call: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x2686df81890 state=finished raised APIConnectionError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:110\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[1;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:557\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:538\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    537\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m--> 538\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOllamaError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\main.py:2808\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2802\u001b[0m     api_base \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2803\u001b[0m         litellm\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[0;32m   2804\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m api_base\n\u001b[0;32m   2805\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLLAMA_API_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2806\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2807\u001b[0m     )\n\u001b[1;32m-> 2808\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mbase_llm_http_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# model call logging done inside the class as we make need to modify I/O to fit aleph alpha's requirements\u001b[39;49;00m\n\u001b[0;32m   2823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama_chat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:362\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler.completion\u001b[1;34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client)\u001b[0m\n\u001b[0;32m    360\u001b[0m     sync_httpx_client \u001b[38;5;241m=\u001b[39m client\n\u001b[1;32m--> 362\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mtransform_response(\n\u001b[0;32m    373\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    374\u001b[0m     raw_response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    383\u001b[0m )\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:131\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._make_common_sync_call\u001b[1;34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:961\u001b[0m, in \u001b[0;36mBaseLLMHTTPHandler._handle_error\u001b[1;34m(self, e, provider_config)\u001b[0m\n\u001b[0;32m    960\u001b[0m     error_headers \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m provider_config\u001b[38;5;241m.\u001b[39mget_error_class(\n\u001b[0;32m    962\u001b[0m     error_message\u001b[38;5;241m=\u001b[39merror_text,\n\u001b[0;32m    963\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[0;32m    964\u001b[0m     headers\u001b[38;5;241m=\u001b[39merror_headers,\n\u001b[0;32m    965\u001b[0m )\n",
      "\u001b[1;31mOllamaError\u001b[0m: {\"error\":\"model 'llava' not found\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\textgrad\\engine_experimental\\litellm.py:43\u001b[0m, in \u001b[0;36mLiteLLMEngine._generate_from_single_prompt\u001b[1;34m(self, content, system_prompt, temperature, max_tokens, top_p)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@retry\u001b[39m(wait\u001b[38;5;241m=\u001b[39mwait_random_exponential(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m), stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_from_single_prompt\u001b[39m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28mself\u001b[39m, content: \u001b[38;5;28mstr\u001b[39m, system_prompt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m\n\u001b[0;32m     41\u001b[0m ):\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlite_llm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\textgrad\\engine_experimental\\litellm.py:19\u001b[0m, in \u001b[0;36mLiteLLMEngine.lite_llm_generate\u001b[1;34m(self, content, system_prompt, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_prompt},\n\u001b[0;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: content},\n\u001b[0;32m     17\u001b[0m ]\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\utils.py:1190\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1188\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1189\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\utils.py:1068\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1069\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\main.py:3085\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3084\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2202\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2171\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[0;32m   2172\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exception_provider, error_str),\n\u001b[0;32m   2173\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mcustom_llm_provider,\n\u001b[0;32m   2174\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2175\u001b[0m         request\u001b[38;5;241m=\u001b[39moriginal_exception\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m   2176\u001b[0m     )\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAPIConnectionError\u001b[0m: litellm.APIConnectionError: OllamaException - {\"error\":\"model 'llava' not found\"}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# this also works with\u001b[39;00m\n\u001b[0;32m     13\u001b[0m set_backward_engine(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello, what\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms 3+4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myou are an assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m image_data \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mget(image_url)\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\textgrad\\engine_experimental\\base.py:85\u001b[0m, in \u001b[0;36mEngineLM.generate\u001b[1;34m(self, content, system_prompt, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m sys_prompt_arg \u001b[38;5;241m=\u001b[39m system_prompt \u001b[38;5;28;01mif\u001b[39;00m system_prompt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_from_single_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys_prompt_arg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     88\u001b[0m     has_multimodal_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m content)\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\textgrad\\engine_experimental\\base.py:16\u001b[0m, in \u001b[0;36mcached.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# get string representation from args and kwargs\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mstr\u001b[39m(args) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(kwargs))\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\projects\\Smart-Answer\\venv\\Lib\\site-packages\\tenacity\\__init__.py:419\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x2686df81890 state=finished raised APIConnectionError>]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import httpx\n",
    "import litellm\n",
    "from textgrad.engine_experimental.litellm import LiteLLMEngine\n",
    "from textgrad import  get_engine, set_backward_engine\n",
    "\n",
    "litellm._turn_on_debug()\n",
    "\n",
    "MODEL_NAME = \"ollama/llava\" \n",
    "\n",
    "\n",
    "engine = get_engine(f\"experimental:{MODEL_NAME}\", cache=False)\n",
    "# this also works with\n",
    "set_backward_engine(f\"experimental:{MODEL_NAME}\", cache=False, override=True)\n",
    "\n",
    "engine.generate(content=\"hello, what's 3+4\", system_prompt=\"you are an assistant\")\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "image_data = httpx.get(image_url).content\n",
    "\n",
    "LiteLLMEngine(engine=engine, cache=True).generate(content=[image_data, \"what is this my boy\"], system_prompt=\"you are an assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
